{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- 각자 이 ipynb 파일의 **사본을 생성**하여 과제 Q0~Q3까지 채운 후 해당 파일을 깃허브에 업로드해주세요!"
      ],
      "metadata": {
        "id": "YTOK1lA7Hsz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Sample Code in PyTorch"
      ],
      "metadata": {
        "id": "2msLr-G13YUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 직접 읽어보며 돌려볼 수 있는 **쉬운** 예제 코드~\n",
        "- 코드 간단 설명:\n",
        "  - 길이 12짜리 binary sequence(0/1)를 입력으로 받아서, 시퀀스 안에 1-0-1 pattern이 한 번이라도 등장하면 1, 아니면 0을 맞추는 binary classification을 수행하는 RNN 분류기\n",
        "  - 마지막에는 demo sequence로 예측 + hidden state 변화까지 출력하는 프로그램"
      ],
      "metadata": {
        "id": "5JrL9Fl93d0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "EO04CkcR4bFR"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 정답 label 만드는 함수\n",
        "- 역할:\n",
        "  - sequence(e.g., [1,0,1,0,0,...]) 안에 연속된 3칸이 1-0-1인 구간이 있는지 검사\n",
        "  - 있으면 label=1 / 없으면 label=0"
      ],
      "metadata": {
        "id": "TLXMSKiQ9k6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def has_101_pattern(seq):\n",
        "    for i in range(len(seq) - 2):\n",
        "        if seq[i] == 1 and seq[i+1] == 0 and seq[i+2] == 1:\n",
        "            return 1\n",
        "    return 0"
      ],
      "metadata": {
        "id": "rfrge_Gq8ZFx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습용 data 만드는 PatternDataset 클래스\n",
        "- 깨알 상식) PyTorch에서 Dataset은 \"데이터를 꺼내는 방식\"을 표준화한 클래스~\n",
        "  - 이걸 상속받아서 내가 하고자 하는 task에 부합하는 나만의 커스텀 Dataset 클래스를 만들어서 모델에 먹이는 겁니다 얍얍"
      ],
      "metadata": {
        "id": "OzaGLdAbGUSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset이 하는 일은 \"모델에 넣기 좋은 형태\"로 데이터를 제공하는 것!\n",
        "class PatternDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seq_len=12):\n",
        "        self.data = []\n",
        "        # (입력 시퀀스, 정답 label)을 n_samples개만큼 저장\n",
        "        for _ in range(n_samples):\n",
        "            seq = [random.randint(0, 1) for _ in range(seq_len)]  # 1) seq_len 길이의 random sequence 생성(0/1)\n",
        "            label = has_101_pattern(seq)                          # 2) has_101_pattern(seq)로 정답 label 생성\n",
        "            self.data.append((seq, label))                        # 3) (seq, label)을 self.data에 저장\n",
        "\n",
        "    # Dataset 안에 sample이 몇 개인지 알려주는 매직 메소드!\n",
        "    # 이걸로 보통 DataLoader가 \"전체 크기\"를 알 수 있게 합니다\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    # idx번째 데이터를 꺼내서 pytorch tensor로 변환해주는 매직 메소드\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)         # (T,) = (12,) / embedding은 정수 인덱스를 받기 때문에 dtype으로 long을 사용합니다!\n",
        "        y = torch.tensor(label, dtype=torch.float32)    # scalar / BCEWithLogitsLoss가 float label(0.0/1.0)을 기대하는 편이라 dtype으로 float32를 사용했어요\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "4legZWrO8YJp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RNN 모델 클래스\n",
        "- pytorch에서 모델 클래스는 일반적으로 nn.Module을 상속해서 만들어요~"
      ],
      "metadata": {
        "id": "uTROYjWUINLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=2, embed_dim=8, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)  # 학습 연산을 위해 (0/1) -> '벡터'로 변환\n",
        "        self.rnn = nn.RNN(input_size=embed_dim, hidden_size=hidden_dim, batch_first=True) # sequence를 왼쪽부터 읽으면서 hidden state를 update\n",
        "        self.fc = nn.Linear(hidden_dim, 1) # 마지막 hidden state로 이진 분류 점수(logit) 출력\n",
        "\n",
        "    # 일반 학습/평가용 feedforward 순전파 함수\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        NOTE: B는 batch size, T는 시퀀스의 길이!\n",
        "\n",
        "        input x: (B, T) 0/1 token\n",
        "        return: logits (B,)\n",
        "        \"\"\"\n",
        "        emb = self.embed(x)            # (B, T, E) / 벡터화\n",
        "        out, h_n = self.rnn(emb)       # 각 시점의 hidden 기록인 out: (B, T, H) / 마지막 hidden state인 h_n: (1, B, H)\n",
        "        last_h = h_n[-1]               # (B, H) / 마지막 hidden만 추출\n",
        "        logits = self.fc(last_h)       # (B, H) -> (B, 1)\n",
        "        return logits.squeeze(1)       # (B,) / loss 계산 편하게 하기 위해 주로 이렇게 squeeze()라는 함수를 사용하여 모양을 맞춰줍니다\n",
        "\n",
        "    def forward_with_trace(self, x):\n",
        "        \"\"\"\n",
        "        시각화를 통해 이해할 수 있도록 time step별 hidden(out)과 마지막 예측(logits)을 함께 리턴하는 함수\n",
        "        x: (1, T) 단일 시퀀스만 넣는 것을 권장함\n",
        "        \"\"\"\n",
        "        emb = self.embed(x)            # (1, T, E)\n",
        "        out, h_n = self.rnn(emb)       # out: (1, T, H)\n",
        "        last_h = h_n[-1]               # (1, H)\n",
        "        logits = self.fc(last_h)       # (1, 1)\n",
        "        return logits.squeeze(1), out.squeeze(0)  # logits: (1,), out: (T, H)\n",
        "        # out.squeeze(0) 추가로 한 이유 : batch=1을 넣으면 shape이 (1,T,H)인데 이 batch의 차원(1)을 제거하여 (T,H)로 보기 좋게 만든것!\n",
        "        # (크게 중요한 건 아닌데 그냥 궁금하실까봐,,)"
      ],
      "metadata": {
        "id": "9B39dpKJ7Kl_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 메인 함수: train()\n",
        "- 내부 로직 STEP BY STEP 설명:\n",
        "  1. (train/val) dataset 생성\n",
        "  2. DataLoader로 배치 묶기\n",
        "  3. model / loss function / optimizer 준비\n",
        "  4. epoch 반복하며 train\n",
        "  5. epoch마다 검증(val) 정확도 출력\n",
        "  6. 마지막에 demo sequence 1개 넣어서 확률 출력\n",
        "  7. demo sequence에서 시간별 hidden state 일부 출력"
      ],
      "metadata": {
        "id": "OXV7srCf9Jhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    \"\"\"\n",
        "    <헷갈리는 개념 (코드 지피티 딸깍하지 말고 이젠 꼭 알아두자)>\n",
        "    - Dataset: 데이터 1개를 어떻게 꺼낼지 정의\n",
        "    - DataLoader: 여러 개를 묶어서 batch를 만들고, 섞고, 반복 가능한 형태로 제공\n",
        "    - shuffle : train은 섞어서 학습이 안정적이고 (True) / val은 평가하는 거니까 섞을 필요 없음 (False)\n",
        "    \"\"\"\n",
        "\n",
        "    train_ds = PatternDataset(n_samples=6000, seq_len=12)\n",
        "    val_ds   = PatternDataset(n_samples=1000, seq_len=12)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    # model/loss function/optimizer 준비\n",
        "    model = SimpleRNNClassifier(embed_dim=8, hidden_dim=16).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Epoch train loop\n",
        "    for epoch in range(1, 6):\n",
        "        model.train()                                   # dropout/batch regularization 등의 mode들이 train 모드로 바뀜\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, y in train_loader:                       # batch 단위로 (x, y) 받음\n",
        "            x, y = x.to(device), y.to(device)           # x:(B,T), y:(B,)\n",
        "\n",
        "            logits = model(x)                           # (B,) / forward 실행\n",
        "            loss = criterion(logits, y)                 # scalar값 (정답 y와 예측 점수인 logit을 비교하여 loss값 계산)\n",
        "\n",
        "            optimizer.zero_grad()                       # 이전 gradient를 0으로\n",
        "            loss.backward()                             # backpropagation\n",
        "            optimizer.step()                            # parameter update\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        train_loss = total_loss / len(train_loader.dataset)\n",
        "\n",
        "        # validation\n",
        "        model.eval()                                    # 평가 모드\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():                           # 평가이므로 학습 때와 달리 gradient 계산 하지 않음\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logits = model(x)\n",
        "                probs = torch.sigmoid(logits)\n",
        "                preds = (probs >= 0.5).float()          # 0.5 이상이면 1로 예측하도록 구현\n",
        "                correct += (preds == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        val_acc = correct / total\n",
        "        print(f\"[Epoch {epoch}] train_loss={train_loss:.4f}  val_acc={val_acc:.4f}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # DEMO: hidden state 흐름을 출력해보자!\n",
        "    # ----------------------------\n",
        "    model.eval()\n",
        "\n",
        "    demo_seq = [1,0,1,0,0,0,0,0,0,0,0,0]   # 패턴 101이 있는 입력 시퀀스\n",
        "    demo = torch.tensor([demo_seq], dtype=torch.long).to(device)  # (1,T)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logit, h_trace = model.forward_with_trace(demo)\n",
        "        prob = torch.sigmoid(logit).item()\n",
        "\n",
        "    print(\"\\n=== Demo ===\")\n",
        "    print(\"Sequence:\", demo_seq)\n",
        "    print(\"Final prob(pattern=101):\", round(prob, 4))\n",
        "\n",
        "    # hidden trace 일부 출력(앞 3스텝 + 마지막 3스텝)\n",
        "    h_cpu = h_trace.cpu()  # (T,H)\n",
        "    print(\"\\nHidden state trace (show first 3 and last 3 time steps):\")\n",
        "    for t in list(range(3)) + list(range(len(demo_seq)-3, len(demo_seq))):\n",
        "        vec = h_cpu[t][:6].tolist()  # hidden_dim 16 중 앞 6개만 보기\n",
        "        vec = [round(v, 3) for v in vec]\n",
        "        print(f\"t={t:2d}, x_t={demo_seq[t]} -> h_t[:6]={vec}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAHlsjUj8Ntd",
        "outputId": "474a1a49-ae0d-43a5-e11c-a554bea3d078"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] train_loss=0.5931  val_acc=0.7430\n",
            "[Epoch 2] train_loss=0.5458  val_acc=0.7930\n",
            "[Epoch 3] train_loss=0.4909  val_acc=0.8080\n",
            "[Epoch 4] train_loss=0.3045  val_acc=0.9320\n",
            "[Epoch 5] train_loss=0.1397  val_acc=0.9990\n",
            "\n",
            "=== Demo ===\n",
            "Sequence: [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Final prob(pattern=101): 0.9812\n",
            "\n",
            "Hidden state trace (show first 3 and last 3 time steps):\n",
            "t= 0, x_t=1 -> h_t[:6]=[0.219, 0.926, 0.811, -0.091, 0.256, 0.39]\n",
            "t= 1, x_t=0 -> h_t[:6]=[0.043, 0.694, -0.906, 0.414, -0.987, 0.355]\n",
            "t= 2, x_t=1 -> h_t[:6]=[0.589, 0.916, 0.98, 0.801, 0.799, 0.199]\n",
            "t= 9, x_t=0 -> h_t[:6]=[0.93, 0.008, -0.351, 0.896, -0.592, -0.676]\n",
            "t=10, x_t=0 -> h_t[:6]=[0.934, 0.019, -0.362, 0.9, -0.58, -0.689]\n",
            "t=11, x_t=0 -> h_t[:6]=[0.936, 0.029, -0.372, 0.903, -0.572, -0.7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 위 코드에서 demo_seq 변수를 아래 두 가지로 바꿔서 각각 실행해보세요~\n",
        "  - 패턴 있음: [1,0,1,0,0,0,0,0,0,0,0,0] → 확률 높아야 함\n",
        "  - 패턴 없음: [1,1,0,0,1,1,0,0,1,1,0,0] → 확률 낮아야 함"
      ],
      "metadata": {
        "id": "IkUeto3kjsbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q0. 위 코드의 출력 결과 분석 & 두 가지 입력을 넣었을 때 각각의 결과를 비교 분석하시오."
      ],
      "metadata": {
        "id": "QBSysnq3kNjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans)  \n",
        "학습 과정에서 epoch이 증가함에 따라 train loss는 꾸준히 감소했고, validation accuracy는 최종적으로 약 0.9까지 상승했다. 이를 통해 모델이 안정적으로 학습되었음을 확인할 수 있다.\n",
        "\n",
        "패턴이 있는 시퀀스([1,0,1,0,0,0,0,0,0,0,0,0])를 넣었을 때 모델은 101 패턴이 있을 확률을 0.9812로 높게 예측했다.\n",
        "\n",
        "반면 패턴이 없는 시퀀스([1,1,0,0,1,1,0,0,1,1,0,0])를 넣었을 때는 101 패턴이 있을 확률이 0.3066로 낮게 나왔다."
      ],
      "metadata": {
        "id": "pifQ80I8kZr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Sample code in PyTorch"
      ],
      "metadata": {
        "id": "njt9mJdlkiDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 아래는 위와 동일하게 진행"
      ],
      "metadata": {
        "id": "G8lrNsxJFRGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "kW-QeXmXkhF9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def has_101_pattern(seq):\n",
        "    for i in range(len(seq) - 2):\n",
        "        if seq[i] == 1 and seq[i+1] == 0 and seq[i+2] == 1:\n",
        "            return 1\n",
        "    return 0"
      ],
      "metadata": {
        "id": "XMoSaivQFF7Q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatternDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seq_len=12):\n",
        "        self.data = []\n",
        "        for _ in range(n_samples):\n",
        "            seq = [random.randint(0, 1) for _ in range(seq_len)]\n",
        "            label = has_101_pattern(seq)\n",
        "            self.data.append((seq, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)\n",
        "        y = torch.tensor([label], dtype=torch.float)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "uo-gXIZOFF5Q"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 여기서부터 LSTM 모델 클래스"
      ],
      "metadata": {
        "id": "0hzsnnU5FL2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=2, embed_dim=8, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)  # (0/1) -> 벡터로 변환\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)                         # (batch, seq_len, embed_dim)\n",
        "        out, (h_n, c_n) = self.lstm(emb)            # out: (batch, seq_len, hidden_dim)\n",
        "                                                    # h_n: (num_layers, batch, hidden_dim)\n",
        "                                                    # c_n: (num_layers, batch, hidden_dim)\n",
        "\n",
        "        last_h = h_n[-1]                            # (batch, hidden_dim)  마지막 layer의 마지막 hidden\n",
        "        logit = self.fc(last_h)                     # (batch, 1)\n",
        "        return logit, out, (h_n, c_n)\n"
      ],
      "metadata": {
        "id": "FyAeyB5sFF2b"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN과의 차이점??\n",
        "\n",
        "1. nn.RNN -> nn.LSTM\n",
        "2. lSTM은 hidden state(h) 말고도 cell state(c)가 추가되었다는 점\n",
        "3. forward 결과에 따른 형태? (output, (h_n, c_n))"
      ],
      "metadata": {
        "id": "OAsdRO6FFUe-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 메인 함수 : train()\n",
        "- 내부 로직 step by step 설명:\n",
        "  1. train, val dataset 생성\n",
        "  2. DataLoader로 배치 묶기\n",
        "  3. model, loss function, optimizer 준비\n",
        "  4. epoch 반복하며 train\n",
        "  5. epoch마다 검증 정확도 출력\n",
        "  6. 마지막에 demo seq 하나 넣어서 확률 출력\n",
        "  7. demo seq에서 시간별 hidden state 출력"
      ],
      "metadata": {
        "id": "A5EtTq-8FVOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    train_ds = PatternDataset(n_samples=6000, seq_len=12)\n",
        "    val_ds   = PatternDataset(n_samples=1000, seq_len=12)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    model = SimpleLSTMClassifier(vocab_size=2, embed_dim=8, hidden_dim=16).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()  # logit을 바로 넣는 BCE\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    n_epochs = 5\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # ---- train ----\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)               # x:(B,12), y:(B,1)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logit, _, _ = model(x)                           # logit:(B,1)\n",
        "            loss = criterion(logit, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(train_ds)\n",
        "\n",
        "        # ---- val ----\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logit, _, _ = model(x)\n",
        "                prob = torch.sigmoid(logit)                  # (B,1)\n",
        "                pred = (prob >= 0.5).float()                 # (B,1)\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"[Epoch {epoch:02d}] loss={avg_loss:.4f} | val_acc={acc:.4f}\")\n",
        "\n",
        "    # ---- demo ----\n",
        "    model.eval()\n",
        "\n",
        "    demo_seq = [1,0,1,0,0,0,1,1,0,0,0,0] # 패턴 없음 -> 확률 낮아야 함\n",
        "\n",
        "    x_demo = torch.tensor(demo_seq, dtype=torch.long).unsqueeze(0).to(device)  # (1, 12)\n",
        "    logit, out_all, (h_n, c_n) = model(x_demo)\n",
        "\n",
        "    prob = torch.sigmoid(logit).item()\n",
        "    print(\"\\n--- DEMO ---\")\n",
        "    print(\"demo_seq:\", demo_seq)\n",
        "    print(f\"pred_prob(pattern=1): {prob:.4f}\")\n",
        "\n",
        "    # 시간별 hidden state 일부 출력\n",
        "    # out_all: (1, seq_len, hidden_dim)  -> time step별 hidden이 들어있음 (LSTM의 output)\n",
        "    out_all = out_all.squeeze(0).detach().cpu()  # (seq_len, hidden_dim)\n",
        "\n",
        "    print(\"\\n[time step별 hidden state 앞 6개 차원만 출력]\")\n",
        "    for t in range(out_all.size(0)):\n",
        "        h_t = out_all[t, :6].numpy()\n",
        "        print(f\"t={t:02d}, x={demo_seq[t]} -> h_t[:6]={h_t}\")\n",
        "\n",
        "    # (참고) 마지막 hidden/cell state도 같이 보기\n",
        "    last_h = h_n[-1].squeeze(0).detach().cpu()    # (hidden_dim,)\n",
        "    last_c = c_n[-1].squeeze(0).detach().cpu()    # (hidden_dim,)\n",
        "    print(\"\\n[마지막 state 요약]\")\n",
        "    print(\"last_h[:6] =\", last_h[:6].numpy())\n",
        "    print(\"last_c[:6] =\", last_c[:6].numpy())\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "stLhRCNMFF0L"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "돌려돌려"
      ],
      "metadata": {
        "id": "1QxJeYuWFcOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSe_IZ-DFFyD",
        "outputId": "b731dab9-ec7e-40f0-e21d-5ab3d9abed82"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 01] loss=0.6127 | val_acc=0.7520\n",
            "[Epoch 02] loss=0.5228 | val_acc=0.7990\n",
            "[Epoch 03] loss=0.4973 | val_acc=0.8120\n",
            "[Epoch 04] loss=0.3945 | val_acc=0.8950\n",
            "[Epoch 05] loss=0.1981 | val_acc=0.9940\n",
            "\n",
            "--- DEMO ---\n",
            "demo_seq: [1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n",
            "pred_prob(pattern=1): 0.9863\n",
            "\n",
            "[time step별 hidden state 앞 6개 차원만 출력]\n",
            "t=00, x=1 -> h_t[:6]=[ 0.31844246 -0.16816589  0.29736707 -0.1834072   0.36026838 -0.25030133]\n",
            "t=01, x=0 -> h_t[:6]=[ 0.26735702  0.37175736 -0.5733442   0.006842   -0.0780103  -0.25567392]\n",
            "t=02, x=1 -> h_t[:6]=[ 0.65055454  0.5064577   0.1493449  -0.43524858  0.5101981  -0.5981966 ]\n",
            "t=03, x=0 -> h_t[:6]=[ 0.62915015  0.6647909   0.11482345 -0.5866819  -0.11230166 -0.6739552 ]\n",
            "t=04, x=0 -> h_t[:6]=[ 0.7521228   0.67247236  0.5123706  -0.8118383  -0.609203   -0.78979814]\n",
            "t=05, x=0 -> h_t[:6]=[ 0.80302167  0.6596158   0.57767516 -0.88464224 -0.71054256 -0.78641367]\n",
            "t=06, x=1 -> h_t[:6]=[ 0.91461647  0.6370621   0.49107596 -0.8706266   0.21127762 -0.8780321 ]\n",
            "t=07, x=1 -> h_t[:6]=[ 0.93993753  0.82883465  0.532982   -0.87134534  0.19154187 -0.9140666 ]\n",
            "t=08, x=0 -> h_t[:6]=[ 0.8900876  0.8415742  0.6231813 -0.9216417 -0.2608092 -0.8980995]\n",
            "t=09, x=0 -> h_t[:6]=[ 0.88554704  0.8310893   0.61760235 -0.92998713 -0.59944457 -0.89618826]\n",
            "t=10, x=0 -> h_t[:6]=[ 0.88446695  0.7896655   0.60970247 -0.9364655  -0.7155483  -0.8920979 ]\n",
            "t=11, x=0 -> h_t[:6]=[ 0.88234     0.74313635  0.6065521  -0.93642455 -0.7516862  -0.88496584]\n",
            "\n",
            "[마지막 state 요약]\n",
            "last_h[:6] = [ 0.88234     0.74313635  0.6065521  -0.93642455 -0.7516862  -0.88496584]\n",
            "last_c[:6] = [ 3.3057165  1.3203251  0.7692631 -4.574342  -2.0828817 -2.9093883]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RNN과 비교할 점 :\n",
        "  1. val_acc\n",
        "  2. train_loss\n",
        "  3. demo prob\n",
        "\n",
        "\n",
        "- RNN vs LSTM\n",
        "\n",
        "  현재는 장기기억이 필요 없어서 유사한 상황.\n",
        "  \n",
        "  trade-off 중요성"
      ],
      "metadata": {
        "id": "0XZsF577Fgy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU Sample code in PyTorch"
      ],
      "metadata": {
        "id": "XRjTLGz8FkJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "FZQRzd_yFFwH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def has_101_pattern(seq):\n",
        "    for i in range(len(seq) - 2):\n",
        "        if seq[i] == 1 and seq[i+1] == 0 and seq[i+2] == 1:\n",
        "            return 1\n",
        "    return 0"
      ],
      "metadata": {
        "id": "IyxVJD_HFFtX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatternDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seq_len=12):\n",
        "        self.data = []\n",
        "        for _ in range(n_samples):\n",
        "            seq = [random.randint(0, 1) for _ in range(seq_len)]\n",
        "            label = has_101_pattern(seq)\n",
        "            self.data.append((seq, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)\n",
        "        y = torch.tensor([label], dtype=torch.float)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "Py8J1mxyFFq3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GRU 클래스:\n",
        "  nn.GRU\n",
        "  \n",
        "  LSTM처럼 cell state(c)가 없고, hidden state(h) 하나만 유지\n",
        "\n",
        "  forward 결과로 out, h_n\n",
        "  \n",
        "  out : 모든 time step의 hidden (batch, seq_len, hidden_dim)\n",
        "\n",
        "  h_n : 마지막 hidden (num_layers, batch, hidden_dim)"
      ],
      "metadata": {
        "id": "tXUAnZxhFtM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleGRUClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=2, embed_dim=8, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)                 # (B, T, E)\n",
        "        out, h_n = self.gru(emb)            # out: (B, T, H), h_n: (1, B, H)\n",
        "        last_h = h_n[-1]                    # (B, H)\n",
        "        logit = self.fc(last_h)             # (B, 1)\n",
        "        return logit, out, h_n"
      ],
      "metadata": {
        "id": "n_7qciOPFFoo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gru():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    train_ds = PatternDataset(n_samples=6000, seq_len=12)\n",
        "    val_ds   = PatternDataset(n_samples=1000, seq_len=12)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    model = SimpleGRUClassifier(vocab_size=2, embed_dim=8, hidden_dim=16).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    n_epochs = 5\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # ---- train ----\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logit, _, _ = model(x)\n",
        "            loss = criterion(logit, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(train_ds)\n",
        "\n",
        "        # ---- val ----\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logit, _, _ = model(x)\n",
        "                prob = torch.sigmoid(logit)\n",
        "                pred = (prob >= 0.5).float()\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"[Epoch {epoch}] train_loss={avg_loss:.4f}  val_acc={acc:.4f}\")\n",
        "\n",
        "    # ---- demo ----\n",
        "    model.eval()\n",
        "\n",
        "    demo_seq = [1,0,1,0,0,0,1,1,0,0,0,0]  # 패턴 있음\n",
        "    x_demo = torch.tensor(demo_seq, dtype=torch.long).unsqueeze(0).to(device)  # (1, 12)\n",
        "\n",
        "    logit, out_all, h_n = model(x_demo)\n",
        "    prob = torch.sigmoid(logit).item()\n",
        "\n",
        "    print(\"\\n=== Demo ===\")\n",
        "    print(\"Sequence:\", demo_seq)\n",
        "    print(f\"Final prob(pattern=101): {prob:.4f}\")\n",
        "\n",
        "    # time step별 hidden state 출력 (앞 6개 차원)\n",
        "    out_all = out_all.squeeze(0).detach().cpu()  # (T, H)\n",
        "\n",
        "    print(\"\\nHidden state trace (show first 3 and last 3 time steps):\")\n",
        "    T = out_all.size(0)\n",
        "    for t in list(range(3)) + list(range(T-3, T)):\n",
        "        h_t = out_all[t, :6].numpy()\n",
        "        # 보기 좋게 소수점 3자리로\n",
        "        h_t_fmt = [float(f\"{v:.3f}\") for v in h_t]\n",
        "        print(f\"t={t:2d}, x_t={demo_seq[t]} -> h_t[:6]={h_t_fmt}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "nrZXy-8iFFlr"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_model = train_gru()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARHGR8JkFFjY",
        "outputId": "4c6ffe30-004a-4e84-e76a-ab30d8d48d43"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] train_loss=0.6150  val_acc=0.7670\n",
            "[Epoch 2] train_loss=0.5535  val_acc=0.7710\n",
            "[Epoch 3] train_loss=0.5042  val_acc=0.7920\n",
            "[Epoch 4] train_loss=0.4405  val_acc=0.8330\n",
            "[Epoch 5] train_loss=0.2734  val_acc=0.9690\n",
            "\n",
            "=== Demo ===\n",
            "Sequence: [1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n",
            "Final prob(pattern=101): 0.9744\n",
            "\n",
            "Hidden state trace (show first 3 and last 3 time steps):\n",
            "t= 0, x_t=1 -> h_t[:6]=[-0.309, -0.146, 0.157, -0.169, 0.407, 0.828]\n",
            "t= 1, x_t=0 -> h_t[:6]=[-0.255, 0.676, 0.198, -0.158, -0.013, -0.107]\n",
            "t= 2, x_t=1 -> h_t[:6]=[-0.777, 0.523, 0.61, -0.589, 0.792, 0.919]\n",
            "t= 9, x_t=0 -> h_t[:6]=[-0.89, 0.929, 0.91, -0.904, 0.707, 0.728]\n",
            "t=10, x_t=0 -> h_t[:6]=[-0.879, 0.944, 0.909, -0.901, 0.612, 0.665]\n",
            "t=11, x_t=0 -> h_t[:6]=[-0.88, 0.94, 0.908, -0.903, 0.672, 0.688]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- LSTM vs GRU ?"
      ],
      "metadata": {
        "id": "lnW68WglF24v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM & GRU 과제"
      ],
      "metadata": {
        "id": "gmuBRGxgF5ne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델의 장기기억 성능을 비교하기 위한 코드입니다.\n",
        "\n",
        "코드 중간의 빈칸을 채우면서, 매애앤 아래의 답변을 채워주시면 됩니다.\n",
        "\n",
        "모르면 인공지능을 사용해도 좋지만, sample code로도 풀 수 있으니 최대한 본인의 힘으로 해보면 좋겠습니다 !!"
      ],
      "metadata": {
        "id": "zM42wRXOF8Tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "bItDmp57FyrU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tmi) 11/7은 제 생일입니다. 감사합니다.\n",
        "def set_seed(seed=117):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(117)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QkVj3qZuFyqA",
        "outputId": "dbd5e4c1-21ac-4c28-9394-c6f2a8176e37"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LongMemoryDataset(Dataset):\n",
        "\n",
        "    def __init__(self, n_samples=8000, T=80):\n",
        "        self.T = T\n",
        "        self.data = []\n",
        "        for _ in range(n_samples):\n",
        "            first_bit = random.randint(0, 1)          # 기억해야 할 정보\n",
        "            middle = [random.randint(0, 1) for _ in range(T-2)]\n",
        "            seq = [first_bit] + middle + [2]          # 마지막은 DELIM=2\n",
        "            label = first_bit\n",
        "            self.data.append((seq, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)               # (T,)\n",
        "        y = torch.tensor([label], dtype=torch.float)          # (1,)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "hZrbG1IeFyoJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. lstm 모델 빈칸 채우기"
      ],
      "metadata": {
        "id": "iLi1JgS9GFQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=3, embed_dim=8, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        # TODO : 임베딩 레이어를 선언하세요.\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        # TODO : LSTM 레이어를 선언하세요. (batch_first=True)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T)\n",
        "        # TODO: 임베딩을 통과시키세요.\n",
        "        emb = self.embed(x)\n",
        "        # TODO : LSTM에 넣고 out, (h_n, c_n)을 받으세요.\n",
        "        out, (h_n, c_n) = self.lstm(emb)\n",
        "        # TODO : 마지막 hidden(last_h)을 얻으세요.\n",
        "        last_h = h_n[-1]\n",
        "        logit = self.fc(last_h)\n",
        "        return logit, out, (h_n, c_n)\n"
      ],
      "metadata": {
        "id": "HWz4-GNaFymh"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. gru 모델 빈칸 채우기"
      ],
      "metadata": {
        "id": "U3dXxnYlGGih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=3, embed_dim=8, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        # TODO : 임베딩 레이어\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        # TODO : GRU 레이어 (batch_first=True)\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T)\n",
        "        # TODO : 임베딩\n",
        "        emb = self.embed(x)\n",
        "        # TODO : GRU forward로 out, h_n 받기\n",
        "        out, h_n = self.gru(emb)\n",
        "        # TODO : 마지막 hidden\n",
        "        last_h = h_n[-1]\n",
        "        logit = self.fc(last_h)\n",
        "        return logit, out, h_n\n"
      ],
      "metadata": {
        "id": "SXYh1847Fyko"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 루프 만들기"
      ],
      "metadata": {
        "id": "ogOE47LuGP7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=6, lr=1e-3, device=\"cpu\", tag=\"\"):\n",
        "    model = model.to(device)\n",
        "    # TODO : loss 함수 선언 (BCEWithLogitsLoss)\n",
        "    crit = nn.BCEWithLogitsLoss()\n",
        "    # TODO : optimizer 선언 (Adam)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        n = 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # TODO : gradient 초기화\n",
        "            opt.zero_grad()\n",
        "            out = model(x)\n",
        "            logit = out[0] if isinstance(out, (tuple, list)) else out\n",
        "\n",
        "            # TODO : loss 계산\n",
        "            loss = crit(logit, y)\n",
        "            # TODO : backprop\n",
        "            loss.backward()\n",
        "            # TODO : optimizer step\n",
        "            opt.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            n += x.size(0)\n",
        "\n",
        "        train_loss = total_loss / n\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                out = model(x)\n",
        "                logit = out[0] if isinstance(out, (tuple, list)) else out\n",
        "\n",
        "                # TODO : prob = sigmoid(logit)\n",
        "                prob = torch.sigmoid(logit)\n",
        "                # TODO : pred = (prob >= 0.5)\n",
        "                pred = (prob >= 0.5).float()\n",
        "\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        val_acc = correct / total\n",
        "        print(f\"{tag}[Epoch {ep}] train_loss={train_loss:.4f}  val_acc={val_acc:.4f}\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ELMItPqMGRG2"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 그대로 실행하시면 됩니다.\n",
        "\n",
        "T = 300\n",
        "train_ds = LongMemoryDataset(n_samples=8000, T=T)\n",
        "val_ds   = LongMemoryDataset(n_samples=2000, T=T)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "lstm = LSTMClassifier(vocab_size=3, embed_dim=8, hidden_dim=32)\n",
        "gru  = GRUClassifier(vocab_size=3, embed_dim=8, hidden_dim=32)\n",
        "\n",
        "print(\"=== LSTM ===\")\n",
        "lstm = train_model(lstm, train_loader, val_loader, epochs=6, lr=1e-3, device=device, tag=\"LSTM \")\n",
        "\n",
        "print(\"\\n=== GRU ===\")\n",
        "gru = train_model(gru, train_loader, val_loader, epochs=6, lr=1e-3, device=device, tag=\"GRU  \")"
      ],
      "metadata": {
        "id": "MqgsERicGRFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f1b6a3-bded-41cc-c256-4a10f81b1bf8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LSTM ===\n",
            "LSTM [Epoch 1] train_loss=0.6938  val_acc=0.5155\n",
            "LSTM [Epoch 2] train_loss=0.6934  val_acc=0.4845\n",
            "LSTM [Epoch 3] train_loss=0.6932  val_acc=0.4960\n",
            "LSTM [Epoch 4] train_loss=0.6935  val_acc=0.4910\n",
            "LSTM [Epoch 5] train_loss=0.6935  val_acc=0.5155\n",
            "LSTM [Epoch 6] train_loss=0.6933  val_acc=0.5070\n",
            "\n",
            "=== GRU ===\n",
            "GRU  [Epoch 1] train_loss=0.6938  val_acc=0.5095\n",
            "GRU  [Epoch 2] train_loss=0.6936  val_acc=0.4955\n",
            "GRU  [Epoch 3] train_loss=0.6935  val_acc=0.4930\n",
            "GRU  [Epoch 4] train_loss=0.6933  val_acc=0.4870\n",
            "GRU  [Epoch 5] train_loss=0.6934  val_acc=0.5100\n",
            "GRU  [Epoch 6] train_loss=0.6934  val_acc=0.5025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. LSTM과 GRU의 차이점에 대해서 간략하게 서술해주세요."
      ],
      "metadata": {
        "id": "M8o9PlFmGZMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans)  \n",
        "LSTM과 GRU는 모두 RNN의 한계를 개선하기 위해 제안된 모델이다.  \n",
        "\n",
        "LSTM은 Input Gate, Forget Gate, Output Gate의 세 가지 게이트와 cell state를 사용해 정보를 조절한다. 이로 인해 긴 시퀀스의 정보를 안정적으로 유지할 수 있지만 파라미터 수가 많고 연산 비용이 크다는 단점이 있다.\n",
        "\n",
        "반면 GRU는 Input Gate와 Forget Gate를 하나로 합친 Update Gate와 Reset Gate만을 사용하며 cell state를 별도로 두지 않는다. 구조가 단순해 파라미터 수가 적고 학습 속도가 빠르다는 장점이 있지만 상황에 따라 LSTM보다 복잡한 패턴을 표현하는 데 한계가 있을 수 있다."
      ],
      "metadata": {
        "id": "SfE8A2EFGeHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. T=80에서 LSTM과 GRU의 학습 곡선을 비교하고, 어느 쪽이 더 안정적으로 수렴했는지 서술해주세요."
      ],
      "metadata": {
        "id": "PRfrs3zGGiDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans)  \n",
        "T=80 환경에서는 두 모델 모두 train loss가 약 0.69 수준에서 거의 감소하지 않았고 validation accuracy 또한 0.49~0.51 범위에서 크게 벗어나지 않았다. 전반적으로 두 모델 간의 성능 차이는 크지 않았으며, 어느 한쪽이 명확하게 더 안정적으로 수렴했다고 보기는 어렵다."
      ],
      "metadata": {
        "id": "IjAVyZcEGiBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. T를 80 → 150 → 300 순으로 늘려서 각각 실행해보고, 어떤 모델이 성능을 더 잘 유지하는지, 왜 그런 것 같은지를 서술해주세요."
      ],
      "metadata": {
        "id": "nWhHz27cGiAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans)  \n",
        "T가 증가할수록 장기 의존성을 더 잘 처리할 수 있는 LSTM이 GRU보다 성능을 더 잘 유지할 것이라고 예상했으나 실제 실험 결과 T를 150, 300으로 늘려도 두 모델 모두 validation accuracy가 약 0.5 수준에 머물렀고, 뚜렷한 성능 차이는 나타나지 않았다. 즉, 이번 실험 설정에서는 LSTM과 GRU 모두 긴 시퀀스에서 학습이 충분히 이루어지지 않아 결과적으로 비슷한 성능을 보였다."
      ],
      "metadata": {
        "id": "OpLgdVqOGh66"
      }
    }
  ]
}